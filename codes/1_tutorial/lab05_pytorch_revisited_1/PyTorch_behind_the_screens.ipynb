{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_behind_the_screens.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HL7RlV1E05zw","colab_type":"text"},"source":["# PyTorch - Revisited\n","\n","In the previous lab you implemented backpropagation for a simple neural network, just using Numpy. This lab you will learn how PyTorch could have saved you a lot of time.\n"]},{"cell_type":"code","metadata":{"id":"bUdyNIWFyZ5D","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqWf5BWX8Zdz","colab_type":"text"},"source":["### PyTorch Autograd\n","To compute the gradient of the loss function w.r.t. all the model parameters you had to manually check how these parameters were involved in computing the neural network's output. You saw how computing these gradients basically came down to applying the chain rule. Recall your sigmoid implementation from the previous lab assignment:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"MdtGc7jk8WLY","colab_type":"code","colab":{}},"source":["def sigmoid(X):\n","    return 1 / (1 + np.exp(-X))\n","\n","\n","def dsigmoid(X):\n","    sig=sigmoid(X)\n","    return sig * (1 - sig)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TR6-tqTAHjXd","colab_type":"text"},"source":["In PyTorch, sigmoid is already defined and would look something like this:"]},{"cell_type":"code","metadata":{"id":"p5F3MQm88W9q","colab_type":"code","colab":{}},"source":["from torch.autograd import Function\n","\n","class SigmoidFunction(Function):\n","\n","    @staticmethod\n","    def forward(ctx, x):\n","        sigmoid = 1 / (1 + torch.exp(-x))\n","        ctx.save_for_backward(sigmoid)\n","        return sigmoid\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        sigmoid, = ctx.saved_tensors\n","        return sigmoid * (1 - sigmoid) * grad_output\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zL0UmzKAHxdo","colab_type":"text"},"source":["It contains a function that defines what happens during the forward pass, and a function that defines how to compute the gradient during the backward pass. Also, it stores relevant information that was computed during the forward pass so it can be reused during the backward pass. In the Numpy example, sigmoid(x) had to be recomputed when computing the gradient. Here it is stored in a context argument.\n","\n","Many commonly used functions have already been defined in PyTorch and the code you usually write are just compositions of these functions. PyTorch thus already knows how to compute the gradients for the models you build! It is very rare that you would have to define the gradient of any function!\n"]},{"cell_type":"markdown","metadata":{"id":"zuf7USaZMacn","colab_type":"text"},"source":["Let's look at an example. Suppose that, for simplicity, we take our model to be a single neuron of fixed size:"]},{"cell_type":"code","metadata":{"id":"uIu_fWJt8WAW","colab_type":"code","colab":{}},"source":["class Neuron:\n","\n","  def __init__(self):\n","    self.weights = torch.randn(5)\n","    self.bias = torch.randn(1)\n","  \n","  def forward(self, x):\n","    return torch.sigmoid(torch.sum(self.weights * x) + self.bias)\n","\n","\n","neuron = Neuron()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8UkN-A5NO_Ij","colab_type":"text"},"source":["Now, when given some labeled data, PyTorch can compute the gradient of a loss function w.r.t. the model parameters without explicitly having to define how this need to be done."]},{"cell_type":"code","metadata":{"id":"Nhe9meE-NPvH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3c3751dc-ddd2-421f-a98c-363f28b26246","executionInfo":{"status":"ok","timestamp":1573139116666,"user_tz":-60,"elapsed":860,"user":{"displayName":"Ron van Bree","photoUrl":"","userId":"10574495138637938052"}}},"source":["# Define a loss function\n","mse = torch.nn.MSELoss()\n","\n","# Create a random (data, label) pair\n","data, label = torch.randn(5), torch.randn(1)\n","\n","# Compute the loss\n","loss = mse(neuron.forward(data), label)\n","\n","# Perform a backward pass to compute the gradients\n","loss.backward()\n","\n","# Print the gradients corresponding to the model weights\n","neuron.weights.grad\n"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.0117,  0.0813, -0.0145,  0.1313,  0.0212])"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"wOOBG4SoQveU","colab_type":"text"},"source":["This gave an error! This is because does not know which variables are model parameters for which the gradient should be computed. This has to be indicated explicitly, otherwise PyTorch would have to keep track of too many redundant computations. To do this we have to change our model to:"]},{"cell_type":"code","metadata":{"id":"FvUFLtotRdgu","colab_type":"code","colab":{}},"source":["class Neuron:\n","\n","  def __init__(self):\n","    self.weights = torch.randn(5, requires_grad=True)  # When creating the model weights, explicitly mention they need \n","    self.bias = torch.randn(1, requires_grad=True)\n","  \n","  def forward(self, x):\n","    return torch.sigmoid(torch.sum(self.weights * x) + self.bias)\n","  \n","  def parameters(self):\n","    return [self.weights, self.bias]\n","\n","\n","neuron = Neuron()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IqD3Ousw04ED","colab_type":"text"},"source":["Now, with the new model definition, try running the same cell again. The gradients will now be computed."]},{"cell_type":"markdown","metadata":{"id":"mE2jyTJ-S5W2","colab_type":"text"},"source":["### PyTorch Optimizer\n","The computed gradients can be used to optimize the model parameters:"]},{"cell_type":"code","metadata":{"id":"_cAJIDvoTQUi","colab_type":"code","colab":{}},"source":["optimizer = torch.optim.SGD(neuron.parameters(), lr=0.1)\n","\n","print(neuron.weights)\n","optimizer.step()\n","print(neuron.weights)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VjkeCQ2SNCC","colab_type":"text"},"source":["### PyTorch Module"]},{"cell_type":"markdown","metadata":{"id":"i0UudVZNSQFy","colab_type":"text"},"source":["To make things even easier, PyTorch contains a Module class that keeps track of which attributes are model parameters. This becomes useful when you have to build larger models. By calling the .parameters() function you obtain all model parameters. Modules that are attributes of another Module will automatically be registered as part of the model. The following implementations are functionally equivalent to the above."]},{"cell_type":"code","metadata":{"id":"2lQ1AUAWVgHa","colab_type":"code","colab":{}},"source":["from torch.nn import Module, Parameter\n","\n","class Neuron(Module):\n","\n","  def __init__(self):\n","    super(Neuron, self).__init__()\n","    self.weights = Parameter(torch.randn(5))  # Parameters are automatically assumed to require a gradient\n","    self.bias = Parameter(torch.randn(1))\n","  \n","  def forward(self, x):\n","    return torch.sigmoid(torch.sum(self.weights * x) + self.bias)\n","\n","\n","neuron = Neuron()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NA5YO5uXgMt","colab_type":"code","colab":{}},"source":["from torch.nn import Module, Linear\n","\n","class Neuron(Module):\n","\n","  def __init__(self):\n","    super(Neuron, self).__init__()\n","    self.linear = Linear(5, 1, bias=True)  # Linear is a Module for dense layers. It's weights are automatically registered as parameters and assumed to require a gradient\n","  \n","  def forward(self, x):\n","    return torch.sigmoid(self.linear(x))\n","\n","\n","neuron = Neuron()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZwZ26soYxDg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"7a174c8b-7071-4b6e-c727-68ceb385c5b8","executionInfo":{"status":"ok","timestamp":1573138968005,"user_tz":-60,"elapsed":693,"user":{"displayName":"Ron van Bree","photoUrl":"","userId":"10574495138637938052"}}},"source":["torch.nn.ParameterList(neuron.parameters())"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ParameterList(\n","    (0): Parameter containing: [torch.FloatTensor of size 5]\n","    (1): Parameter containing: [torch.FloatTensor of size 1]\n",")"]},"metadata":{"tags":[]},"execution_count":43}]}]}