{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OL5Pt0sgKcKP"
   },
   "source": [
    "# PyTorch - How to use the GPU\n",
    "Most of the computations that are done when training your deep learning model will consist of matrix multiplications. GPUs are optimized for these type of computations and can therefore greatly decrease the time it takes to train your model. When your models become larger, this can save hours or even days of training. This lab explains how to use the GPU in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHLYobqxNVit"
   },
   "source": [
    "We recommend you use the GPU provided by Colab instead of the one in your laptop, as these are much better and also simplify the tool installation. Also, PyTorch makes use of CUDA, which is a platform for general purpose computing on GPUs. Not all GPUs are compatible with CUDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82PYF9HWRqx-"
   },
   "source": [
    "PyTorch tensors are either allocated on the CPU or GPU. Tensors located on the CPU cannot interact with those on the GPU and vice versa. This is something you need to keep track of when programming your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uy0bptrIvQu"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDINCK06uyuR"
   },
   "source": [
    "If you are working in Colab, try enabling the GPU by, in the menu above, selecting 'Runtime' and 'Change runtime type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1573546259141,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     },
     "user_tz": -60
    },
    "id": "cnnUrjaEIvTe",
    "outputId": "1f7cb668-142a-4caa-f4aa-edabf7a6373c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('GPU is available!')\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  print('GPU is not available!')\n",
    "  device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmMBMJDOv3oc"
   },
   "source": [
    "Next, we allocate some tensors on both the CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cL_N6UEYIx7q"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tensor_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))   \u001b[38;5;66;03m# CPU tensor\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tensor_2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GPU tensor\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tensor_3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)                \u001b[38;5;66;03m# Initialized on the device being used\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tensor_4 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m)                               \u001b[38;5;66;03m# By default, tensors are initialized as CPU tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "tensor_1 = torch.randn(4, device=torch.device('cpu'))   # CPU tensor\n",
    "tensor_2 = torch.randn(4, device=torch.device('cuda'))  # GPU tensor\n",
    "tensor_3 = torch.randn(4, device=device)                # Initialized on the device being used\n",
    "tensor_4 = torch.randn(4)                               # By default, tensors are initialized as CPU tensors\n",
    "tensor_5 = torch.FloatTensor([1,2,3,4])                 # CPU tensor\n",
    "tensor_6 = torch.cuda.FloatTensor([1,2,3,4])            # GPU tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4CyTvazqvW6"
   },
   "source": [
    "Since the following tensors are not allocated on the same device, they cannot be multiplied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEX2WklWIx5C"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtensor_1\u001b[49m \u001b[38;5;241m*\u001b[39m tensor_2\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor_1' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_1 * tensor_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQABtpIuGiXt"
   },
   "source": [
    "The .to() function can be used to move a tensor from one device to another. However, the usage of this function should be minimized, as it is a relatively expensive operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1573546281031,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     },
     "user_tz": -60
    },
    "id": "EG0EJ6PWIvOU",
    "outputId": "dc971e85-53cb-4611-a2c2-c2b18832d972"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3309,  3.1425,  0.6378,  0.0419], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_3 * tensor_4.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kbm3qRw7r4t0"
   },
   "source": [
    "Since model parameters are tensors, they are allocated on either the CPU or GPU as well. Therefore, models allocated on some device cannot process data on another device. Using the .to() function it is possible to move entire models between devices as well (if they extend torch.nn.Module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcT6OvS7H7Jc"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "# Define a small Module example to move between devices\n",
    "class Neuron(Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Neuron, self).__init__()\n",
    "    self.weights = Parameter(torch.randn(4))\n",
    "    self.bias = Parameter(torch.randn(1))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return torch.sigmoid(torch.sum(self.weights * x) + self.bias)\n",
    "\n",
    "\n",
    "neuron = Neuron().to(device)\n",
    "neuron.forward(tensor_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLRDKibr0TCQ"
   },
   "source": [
    "### Exercise\n",
    "Try modifying the code below such that the training loop uses the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y538VmgqH7PP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DumY48me1lFM"
   },
   "outputs": [],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, output_size):\n",
    "        super(TwoLayerNet , self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1, bias=False)\n",
    "        self.layer2 = nn.Linear(hidden_size1, output_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = F.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        return F.softmax(z, dim=1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7115,
     "status": "ok",
     "timestamp": 1573549819212,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     },
     "user_tz": -60
    },
    "id": "4ZOm33Ek0sMz",
    "outputId": "f8250875-252b-4f80-b2b3-dfc745319052"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:02, 3869971.49it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 57378.86it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 955833.47it/s]                             \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data_mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 21722.03it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data_mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda x: x.squeeze()),\n",
    "                                ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data_mnist',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform\n",
    "                                      )\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data_mnist',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdfdpqBT0sKH"
   },
   "outputs": [],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "bs=128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=bs,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True\n",
    "                                          )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=bs,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTRu0f-m0sD9"
   },
   "outputs": [],
   "source": [
    "# Modify the following code such that it uses the GPU\n",
    "\n",
    "net=TwoLayerNet(784, 64, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(net.parameters() , lr=0.1)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "\n",
    "  for i, (minibatch_data, minibatch_label) in enumerate(trainloader):\n",
    "\n",
    "      # Set dL/dU, dL/dV, dL/dW to be filled with zeros\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      #reshape the minibatch\n",
    "      inputs = minibatch_data.view(bs, 784)\n",
    "\n",
    "      # forward the minibatch through the net  \n",
    "      prob=net(inputs) \n",
    "      \n",
    "      # Compute the average of the losses of the data points in the minibatch\n",
    "      loss = criterion(prob , minibatch_label) \n",
    "      \n",
    "      # backward pass to compute dL/dU, dL/dV and dL/dW    \n",
    "      loss.backward()\n",
    "      \n",
    "      # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1573550471372,
     "user": {
      "displayName": "Ron van Bree",
      "photoUrl": "",
      "userId": "10574495138637938052"
     },
     "user_tz": -60
    },
    "id": "c4zHiewpH7Mt",
    "outputId": "70f0820b-0e99-4ad7-d138-aae03fed8031"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence scores:\n",
      "0: 0.008735028095543385\n",
      "1: 2.560830944275949e-05\n",
      "2: 0.0009293326875194907\n",
      "3: 0.014074495062232018\n",
      "4: 0.0034834493417292833\n",
      "5: 0.7097895741462708\n",
      "6: 0.0015291203744709492\n",
      "7: 9.452593076275662e-06\n",
      "8: 0.24014447629451752\n",
      "9: 0.021279430016875267\n",
      "\n",
      "Label with highest confidence score: 5\n"
     ]
    }
   ],
   "source": [
    "# This code does not need to be modified!\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# choose a picture at random\n",
    "im_minibatch, label_minibatch = iter(testloader).next()\n",
    "im, label = im_minibatch[0].cpu(), label_minibatch[0].cpu()\n",
    "\n",
    "# Function to show an image tensor\n",
    "def show(X):\n",
    "    if X.dim() == 3 and X.size(2) == 3:\n",
    "        plt.imshow(X.numpy())\n",
    "        plt.show()\n",
    "    elif X.dim() == 2:\n",
    "        plt.imshow(   X.numpy() , cmap='gray'  )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('WRONG TENSOR SIZE')\n",
    "\n",
    "# diplay the picture\n",
    "show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "prob = net.cpu()(im.view(1,784)) \n",
    "\n",
    "print('Confidence scores:\\n' + '\\n'.join(['{}: {}'.format(i, p.item()) for i, p in enumerate(prob.squeeze())]))\n",
    "\n",
    "print('\\nLabel with highest confidence score: {}'.format(torch.argmax(prob).item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#It uses the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the following code such that it uses the GPU\n",
    "\n",
    "net=TwoLayerNet(784, 64, 10)\n",
    "# Determining GPU Availability:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_avaialble() else \"cpu\")\n",
    "# Moving the Network to GPU\n",
    "net.to(device) # (This line moves the neuaral network to the GPU)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(net.parameters() , lr=0.1)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "\n",
    "  for i, (minibatch_data, minibatch_label) in enumerate(trainloader):\n",
    "      # Move data and labels to GPU\n",
    "      minibatch_data, minibatch_label = minibatch_data.to(device), minibatch_label.to(device)\n",
    "      # Set dL/dU, dL/dV, dL/dW to be filled with zeros\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      #reshape the minibatch\n",
    "      bs = minibatch_data.size(0) # Ensuring the Batch size Handling:\n",
    "      inputs = minibatch_data.view(bs, 784)\n",
    "\n",
    "      # forward the minibatch through the net  \n",
    "      prob=net(inputs) \n",
    "      \n",
    "      # Compute the average of the losses of the data points in the minibatch\n",
    "      loss = criterion(prob , minibatch_label) \n",
    "      \n",
    "      # backward pass to compute dL/dU, dL/dV and dL/dW    \n",
    "      loss.backward()\n",
    "      \n",
    "      # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "      optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPU Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
